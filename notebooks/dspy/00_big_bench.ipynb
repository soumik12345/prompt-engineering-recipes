{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving BIG-Bench Hard tasks Using DSPy and Weave\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/soumik12345/prompt-engineering-recipes/blob/main/notebooks/dspy/00_big_bench.ipynb)\n",
    "\n",
    "This notebook demonstrates how we can solve the causal judgement task from the [BIG-bench Hard](https://github.com/suzgunmirac/BIG-Bench-Hard?tab=readme-ov-file) benchmark by optimizing our prompting strategy using [DSPy](https://dspy-docs.vercel.app) and evaluating our prompts using [Weave](https://wandb.me/weave)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU dspy weave datasets rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "api_key = getpass(\"Enter you OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "import weave\n",
    "from datasets import load_dataset\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import dspy\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the BIG-Bench Hard Dataset\n",
    "\n",
    "The [BIG-bench](https://github.com/google/BIG-bench) (Beyond the Imitation Game Benchmark) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities consisting of more than 200 tasks. [BIG-bench Hard](https://github.com/suzgunmirac/BIG-Bench-Hard) is a suite of 23 challenging BIG-Bench tasks for which prior language model evaluations did not outperform the average human-rater.\n",
    "\n",
    "We're gonna load this dataset from HuggingFace Hub, split into training and validation sets, and [publish](https://wandb.github.io/weave/guides/core-types/datasets) them on Weave, this would let us version the datasets, and also use [`weave.Evaluation`](https://wandb.github.io/weave/guides/core-types/evaluations/) to evaluate our prompting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(task: str = \"causal_judgement\", num_train_examples: int = 50):\n",
    "    # load the BIG-Bench Hard dataset corresponding to the task from Huggingface Hug\n",
    "    dataset = load_dataset(\"maveriq/bigbenchhard\", task)[\"train\"]\n",
    "    \n",
    "    # create the training and validation datasets\n",
    "    rows = [{\"question\": data[\"input\"], \"answer\": data[\"target\"]} for data in dataset]\n",
    "    train_rows = rows[0:num_train_examples]\n",
    "    val_rows = rows[num_train_examples:]\n",
    "\n",
    "    # create the training and validation examples consisting of `dspy.Example` objects\n",
    "    dspy_train_examples = [dspy.Example(row).with_inputs(\"question\") for row in train_rows]\n",
    "    dspy_val_examples = [dspy.Example(row).with_inputs(\"question\") for row in val_rows]\n",
    "\n",
    "    # publish the datasets to the Weave, this would let us version the data and use for evaluation\n",
    "    weave.publish(weave.Dataset(name=f\"bigbenchhard_{task}_train\", rows=train_rows))\n",
    "    weave.publish(weave.Dataset(name=f\"bigbenchhard_{task}_val\", rows=val_rows))\n",
    "    \n",
    "    return dspy_train_examples, dspy_val_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weave is currently integrated with DSPy, and including `weave.init` at the start of our code lets us automatically trace our DSPy functions which can be explored in the Weave UI. Check out the Weave integration docs for DSPy to learn more: https://wandb.github.io/weave/guides/integrations/dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(project_name=\"dspy-bigbenchhard\")\n",
    "\n",
    "dspy_train_examples, dspy_val_examples = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DSPy Program\n",
    "\n",
    "We're gonna use the `dspy.OpenAI` abstraction to make LLM calls to GPT3.5 Turbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert in the field of causal reasoning. You are to analyze the a given question carefully and answer in `Yes` or `No`.\n",
    "You should also provide a detailed explanation justifying your answer.\n",
    "\"\"\"\n",
    "\n",
    "llm = dspy.OpenAI(model=\"gpt-3.5-turbo\", system_prompt=system_prompt)\n",
    "dspy.settings.configure(lm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSPy is a framework that pushes building new LM pipelines away from manipulating free-form strings and closer to programming (composing modular operators to build text transformation graphs) where a compiler automatically generates optimized LM invocation strategies and prompts from a program.\n",
    "\n",
    "According to the DSPy programming model, first string-based prompting techniques are translated into declarative modules that carry natural-language typed signatures. Then, each module is the parameterized so that it can learn its desired behavior by iteratively bootstrapping useful demonstrations within the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(BaseModel):\n",
    "    query: str = Field(description=\"The question to be answered\")\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    answer: str = Field(description=\"The answer for the question\")\n",
    "    confidence: float = Field(ge=0, le=1, description=\"The confidence score for the answer\")\n",
    "    explanation: str = Field(description=\"The explanation for the answer\")\n",
    "\n",
    "\n",
    "class QuestionAnswerSignature(dspy.Signature):\n",
    "    input: Input = dspy.InputField()\n",
    "    output: Output = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSPy modules are task-adaptive components—akin to neural network layers—that abstract any particular text transformation, in this case returning a structured question-answering result by executing causal reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModule(dspy.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prog = dspy.TypedPredictor(QuestionAnswerSignature)\n",
    "    \n",
    "    @weave.op()\n",
    "    def forward(self, question) -> dict:\n",
    "        return self.prog(input=Input(query=question)).output.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QAModule()\n",
    "\n",
    "prediction = model(dspy_train_examples[0][\"question\"])\n",
    "rich.print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our DSPy Program\n",
    "\n",
    "Now that we have a baseline prompting strategy, let's evaluate it on our validation set using [`weave.Evaluation`](https://wandb.github.io/weave/guides/core-types/evaluations/) on a simple metric that matches the predicted answer with the ground truth. Weave will take each example, pass it through your application and score the output on multiple custom scoring functions. By doing this, you'll have a view of the performance of your application, and a rich UI to drill into individual outputs and scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def weave_evaluation_metric(answer: str, model_output: Output) -> dict:\n",
    "    return {\n",
    "        \"match\": int(answer.lower() == model_output[\"answer\"].lower())\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(\n",
    "    name=\"Naive QAModule\",\n",
    "    dataset=weave.ref(\"bigbenchhard_causal_judgement_val:v0\").get(),\n",
    "    scorers=[weave_evaluation_metric]\n",
    ")\n",
    "await evaluation.evaluate(model.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing our DSPy Program\n",
    "\n",
    "When compiling a DSPy program, we generally invoke a teleprompter, which is an optimizer that takes the program, a training set, and a metric—and returns a new optimized program. In this example, we use the [`BootstrapFewShot`](https://dspy-docs.vercel.app/docs/deep-dive/teleprompter/bootstrap-fewshot#how-bootstrapfewshot-works) teleprompter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def get_optimized_program(model: dspy.Module) -> dspy.Module:\n",
    "    \n",
    "    @weave.op()\n",
    "    def dspy_evaluation_metric(true, prediction, trace=None):\n",
    "        return prediction[\"answer\"].lower() == true.answer.lower()\n",
    "\n",
    "\n",
    "    teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "        metric=dspy_evaluation_metric, \n",
    "        max_bootstrapped_demos=8, \n",
    "        max_labeled_demos=8,\n",
    "    )\n",
    "    return teleprompter.compile(\n",
    "        model, trainset=dspy_train_examples, valset=dspy_val_examples[:10]\n",
    "    )\n",
    "\n",
    "\n",
    "optimized_model = get_optimized_program(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our optimized program (the optimized prompting strategy), let's evaluate it once again on our validation set and compare it with our baseline DSPy program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(\n",
    "    name=\"Optimized QAModule\",\n",
    "    dataset=weave.ref(\"bigbenchhard_causal_judgement_val:v0\").get(),\n",
    "    scorers=[weave_evaluation_metric]\n",
    ")\n",
    "await evaluation.evaluate(optimized_model.forward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
